# -*- coding: utf-8 -*-
"""Deep Learning for Computer Vision with Python and TensorFlow â€“ Complete Course (FreeCodeCamp).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Bku-RXWrzlFm5BDgGtQkWJQ93n8Cc8bv
"""

import tensorflow as tf #importing tensorflow - no need to install due to using Google Colab3
import numpy as np

#Creating 0 dimensional tensor.
tensor_zero_d = tf.constant(4) # Creating a 0-dimensional tensor
print(tensor_zero_d)

#Creating 1 dimensional tensor.
tensor_one_d = tf.constant([2,0,-3,8,90],dtype=tf.float16) #2-dimentional tensor uses array.
#When memory is constrained - use lower precision tensors.
print(tensor_one_d) #Shape is 5 representing 5 elements in the Tensor & type is integer - but it can be a float

#Creating 2 dimensional tensor.
tensor_two_d =  tf.constant([
    [1,2,0],
    [3,5,-1],
    [1,5,6],
    [2,3,8]
]) #Note that nesting [] are required outside of each 1d tensor in the nested array comprising the 2D tensor.
print(tensor_two_d)

#Creating 3D tensor - putting together 2D tensors.
tensor_three_d = tf.constant([
    [
      [1,2,0],
      [3,5,-1]
    ],
    [
      [1,2,0],
      [3,5,-1]
    ],
    [
      [1,2,0],
      [3,5,-1]
    ]
])
print(tensor_three_d)
print(tensor_three_d.shape)

#Constructing a 4D tensor - made by stacking multiple 3D tensors.
tensor_4_d = tf.constant([
    [
      [
        [1,2,0],
        [3,5,-1]
      ],
      [
        [1,2,0],
        [3,5,-1]
      ],
      [
        [1,2,0],
        [3,5,-1]
      ]
  ],
  [
    [
      [1,2,0],
      [3,5,-1]
    ],
    [
      [1,2,0],
      [3,5,-1]
    ],
    [
      [1,2,0],
      [3,5,-1]
    ]
  ]
])
print(tensor_4_d)

#Experimenting with data types
tensor_one_d = tf.constant([2,0,-3,8,90],dtype=tf.float32)
casted_tensor_one_d = tf.cast(tensor_one_d,dtype=tf.int16) #casting tensor switches data types - useful if the neural net has specific requirements e.g. constrained memory.
bool_tensor_one_d = tf.cast(tensor_one_d,dtype=tf.bool) #casting tensor to a boolean.
print(casted_tensor_one_d)
print(bool_tensor_one_d) #0 is a falsy value.

tensor_bool = tf.constant([True,True,False]) #creates a tensor directly using boolean data type.
tensor_string = tf.constant(["Hello","World","This","Is","My","String","Tensor"]) #Creates a tensor using string data type.
print(tensor_string)

#Converting an nparray to a tensor
import numpy as np
np_array = np.array([1,2,4])
print(np_array)
converted_tensor = tf.convert_to_tensor(np_array)
print(converted_tensor)

#eye method constructs batch of matrices
eye_tensor = tf.eye(
    num_rows =5,
    num_columns=None,
    batch_shape=[2], #Number of batches i.e. matricies
    dtype=tf.dtypes.float32,
    name=None
)
print(eye_tensor) #All values of matrix are 0 apart from leading diagonal - it is an identity matrix.

#Fill creates a tensor filled with scalar value - all values take the specified value.
fill_tensor = tf.fill([2,3],9, name=None)
print(fill_tensor)

#Ones method is similar, but fills tensor with all values of 1
ones_tensor = tf.ones([2,3],dtype=tf.float32,name=None)
print(ones_tensor)

#ones_like function
ones_like_tensor = tf.ones_like(fill_tensor) #uses fill_tensor described before & creates ones tensor of same size with same datatype
print(ones_like_tensor)

#tf.zeros method - all elements set to 0.
zeros_tensor = tf.zeros(
    [3,2],
    dtype=tf.dtypes.float32,
    name=None
)
print(zeros_tensor)

#shape method
my_tensor = tf.constant([
    [1,2,0],
    [3,5,-1],
    [1,5,6],
    [2,3,8]
])

tensor_shape = my_tensor.shape
print(tensor_shape)

#rank method - returns rank of tensor
tensor_rank = tf.rank(my_tensor)
print(tensor_rank)

#size method - returns size of a tensor
tensor_size = tf.size(my_tensor)
print(tensor_size) #Has a size of 12 as there are 12 elements in the tensor

#creates random tensors from normal distribution
random_tensor = tf.random.normal(
    [3,2],
    mean=0.0, #use mean & stdev to decide on the type of random values to have in the tensor based on the normal distribution.
    stddev=1.0,
    dtype=tf.dtypes.float32,
    seed=None,
    name=None
)
print(random_tensor) #Prints tensor with random values seed for randomness initialized by system time.

#Creating random values from uniform distribution - in uniform distribution the probability of returning a number at any point in the distribution is equal.
my_uniform_tensor = tf.random.uniform(
    [3,2],
    minval=0,
    maxval=100, #defaults to 1 if not set - see documentation
    dtype=tf.dtypes.float32,
    seed=None,
    name=None
)
print(my_uniform_tensor)

#Tensor indexing - use to slice out parts of the tensor.
indexed_tensor = tf.constant([1,2,3,5,7,5,6,7,645,6,6,5,4,3,6]) #tensor indexing with a 1D tensor

print(indexed_tensor[0:2]) #takes 1st to 2nd element (last number is exclusive)
print(indexed_tensor[0:2+1])
print(indexed_tensor[0:10:2]) #last number is step minval:maxindex:step

index_tensor_2d = tf.constant([
    [1,2,0],
    [3,5,-1],
    [1,5,6],
    [2,3,8]
])

print(index_tensor_2d[0:3,0:2]) #selects firstly rows, then columns (up the stairs & along the coridoor! Backwards to cartesian coordinates!)

#use spread operator ... to select all of the indexes
print(index_tensor_2d[...,1]) #all rows & first column

#Using : to select ommit all values for given layer.
tensor_3_d = tf.constant(
    [
                         [[1,2,0],
                          [3,5,-1]],

                         [[10,2,0],
                          [1,0,2]],

                         [[5,8,0],
                          [2,7,0]],

                         [[2,1,9],
                          [4,-3,32]],
    ]
)

print(tensor_3_d[0, : , : ]) #references index 0 for rows & omitts all values for columns. i.e. row 0, no columns
#This tensor has 3 dimensions - so 3 values required here. The size of the tnesor is 4 x 2 x 3 so these values would select all of the values in the tensor. Kinda like 'amount of stairs, up the stairs & along the corridoor. Can use negative numbers to start counting from the end.

#tf.math functions
#tf.maths.abs function - gives absolute value of a tensor - turns negative into positive, but leaves positive unchanged.

tf.abs(tensor_3_d)

tf.abs(tf.constant([-0.2]))

#tf.abs for complex numbers
#a + bj abs is computed as aqrt(a^2 + b^2)

tensor_complex = tf.constant([-2.25 + 4.75j])
tf.abs(tensor_complex)

tf.sqrt((-2.25)**2 + 4.75**2)

x_1 = tf.constant([5,3,1], dtype = tf.int32)
x_2 = tf.constant([1,2,3], dtype = tf.int32)

tf.add(x_1, x_2) #similar functions for divide, subtract etc

x_3 = tf.constant([7], dtype = tf.int32)
tf.add(x_1, x_3) #In this example, tensors are different shape - the value is added to all values in the larger tensor (it's called broadcasting when smaller tensor is stretched to allow the operation to be carried out).

#tf.max.maximum
tf.maximum(x_1, x_2) #Compares element at each position & returns result.

#tf.argmax - finds indices of maximum value in tensor along specific axis.
max_tensor = tf.constant(
    [[[1,2,3],
    [4,5,6]],
    [[7,8,9],
     [10,11,12]]]
)

print(tf.math.argmax(max_tensor, 0)) #0 argument refers to axis 0.

#tf.math.argmax returns the min value along specific axis.

#tf.math.reduce_[fn] is a set of reducer functions to perform operations along axes in the tensor.

tensor_one = [1,2,3]

print(tf.math.reduce_sum(tensor_one, axis=None, keepdims=False, name=None)) #I expect the output to be 6

tensor_two = [[1,2,3],
            [4,6,1]]

print(tf.math.reduce_sum(tensor_two, axis=None, keepdims=False, name=None)) #I expect the output to be 17

#tf.math.top_k - gets the top x values from the tensor
topk_tensor = [[[1,2,3],[4,5,6]],[[7,8,1],[10,1,0]]]
print(tf.math.top_k(topk_tensor,2,sorted=True,index_type=tf.dtypes.int32,name=None)) #I expect the output to be [10,8]

"""The output was different from what I expected because tf.math.top_k finds the top k values for the last dimention - not the first as I originally assumed. It basically goes through every row."""

#tf.linalg.matmul - matrix multiplication where number of rows must match number of columns.

matmul_tensor_1 = tf.constant([[10,2], #This is a 2 x 2 matrix.
                               [1,4]])

matmul_tensor_2 = tf.constant([[1,2,4], #This is a 2 x 3 matrix
                               [5,2,1]]) #The number of rows in this matrix (2) matches the number of columns in the above matrix (2)

print(tf.linalg.matmul(matmul_tensor_1, matmul_tensor_2))

#tf.linalg.band_part - sets specified subdiagonals to 0

band_tensor =  [[ 1,  2,  0,  0],
                [ 5,  6,  7,  0],
                [ 9, 10, 11, 12],
                [13, 14, 15, 16]]

print(tf.linalg.band_part(band_tensor, -1, -1))
print(tf.linalg.band_part(band_tensor, 1, -1))
print(tf.linalg.band_part(band_tensor, 1, 1))
print(tf.linalg.band_part(band_tensor, 1, -1))

#einsum - performs operations on matrices using i & j as variables to determine what happens to the matrix.
einsum_t1 =    tf.constant([[ 1,  2,  0,  0],
                            [ 5,  6,  7,  0],
                            [ 9, 10, 11, 12],
                            [13, 14, 15, 16]])

einsum_t2 =    tf.constant([[ 4,  2,  0,  0],
                            [ 5,  9,  7,  0],
                            [ 9, 10, 11, 2],
                            [13, 14, 15, 16]])

print(np.einsum('ij,ij -> ij', einsum_t1, einsum_t2)) #Performs a standard multiplication between the 2 matrices (element wise multiplication). i.e. matrix dims ij is multiplied by matrix dims ij to result in a matrix with dims ij
print (tf.einsum('ij,ij -> ij', einsum_t1, einsum_t2))

"""Both of these functions worked when the tensors were defined inside of tf.constnat!"""

#Using einsum to transpose a matrix ij -> ji
print(np.einsum('ij -> ji', einsum_t1)) #transposes the matrix (inverts it).

#tf.gather - takes an input tensor & index tensor, gather elements along specified axis from input tensor based on index tensor.

input_tensor = [[[0,1,2,3],   #In this case, the input tensor is a 3D tensor with dimensions 2x3x4
                 [1,4,7,9],   #Axis 0 is the z-axis with 2 elements.
                 [2,8,2,6]],  #Axis 1 is the y-axis (number of rows) containing 3 elements.
                              #Axis 2 is the x-axis (number of columns) containing 4 elements.
                [[2,5,4,2],
                 [4,6,8,1],
                 [9,7,2,6]]]

index_tensor = [0,1]

print(tf.gather(input_tensor, index_tensor)) #Specifies indices to get from input along axis 0, I expect the result of this to be the original input tensor.
print(tf.gather(input_tensor, index_tensor, axis=1)) #Now axis 1
print(tf.gather(input_tensor, index_tensor, axis=2)) #Now axis 2
#Notice how for each one of these operations, the resulting tensor is still 3D, no matter the axis.

#tf.gather_nd - same as tf.gather but rather than specifying axis, uses multi-dimensional array.

multi_dim_index_t1 = [0]      #gets first element from axis 0
multi_dim_index_t1 = [[0],  #gets from element 0 to 1 in axis 0
                      [1]]

input_tensor = [[[0,1,2,3],
                 [1,4,7,9],
                 [2,8,2,6]],

                [[2,5,4,2],
                 [4,6,8,1],
                 [9,7,2,6]],

                [[3,6,1,0],
                 [4,7,2,8],
                 [1,2,4,3]]]

print(tf.gather(input_tensor, multi_dim_index_t1))

